# ğŸ¤– Borgo-AI
> the readme may or may not be made by borgo-ai
> **Local AI CLI Assistant powered by Llama 3.1**  
> Beautiful, fast, and fully private - runs entirely on your machine.

```
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
 â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—      â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•      â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•       â•šâ•â•  â•šâ•â•â•šâ•â•
```

## âœ¨ Features

- ğŸ¦™ **Local LLM** - Powered by Llama 3.1:8b via Ollama (no API keys needed!)
- ğŸ¨ **Beautiful CLI** - Rich terminal UI with ASCII art, markdown rendering, and themes
- ğŸ” **Web Search** - AI-powered web browsing when it needs current information
- ğŸ§  **Memory System** - Remembers conversations and important facts
- ğŸ“š **RAG Support** - Add documents to a knowledge base for context-aware responses  
- ğŸ¤– **Agent Mode** - ReAct-style reasoning with tool use
- ğŸ‘¥ **Multi-User** - Per-user settings, memory, and conversation history
- ğŸ’¾ **Persistent Storage** - Your chats and memories are saved locally
- âš¡ **Optimized** - Designed for RTX 3060 Ti (8GB VRAM) + 16GB RAM

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ollama** (if not already installed):
    ```bash
    sudo pacman -Syu
    sudo pacman -S ollama
    ```
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Pull the required models**:
   ```bash
   # Main LLM
   ollama pull llama3.1:8b
   
   # Embedding model
   ollama pull nomic-embed-text
   ```

3. **Start Ollama server**:
   ```bash
   ollama serve
   ```

### Installation

```bash
# Clone/navigate to the project
cd borgo-ai

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Run Borgo-AI

```bash
# Interactive mode (recommended)
python main.py chat

# Or ask a single question
python main.py ask "What is the meaning of life?"

# With web search
python main.py ask "What's the latest news about AI?" --browse

# Agent mode
python main.py ask "Research the best Python web frameworks" --agent
```

## ğŸ“– Commands

In interactive mode, use these commands:

| Command | Description |
|---------|-------------|
| `/help` | Show all commands |
| `/new` | Start a new conversation |
| `/history` | Show past conversations |
| `/load <id>` | Load a conversation |
| `/delete <id>` | Delete a conversation |
| `/memory` | Show saved memories |
| `/remember <text>` | Save to long-term memory |
| `/forget <id>` | Delete a memory |
| `/search <query>` | Search the web |
| `/agent <query>` | Use agent mode |
| `/knowledge add <text>` | Add to knowledge base |
| `/knowledge query <text>` | Search knowledge base |
| `/load <filepath>` | Load file into knowledge base |
| `/run <py\|bash> <code>` | Execute code (safe) |
| `/image <filepath>` | View image info |
| `/summarize [text]` | Summarize chat or text |
| `/export [html\|md\|json]` | Export conversation |
| `/user` | Manage users |
| `/settings` | View/change settings |
| `/wipe <all\|chats\|memory>` | Delete data |
| `/clear` | Clear screen |
| `/exit` | Exit borgo-ai |

## âš™ï¸ Settings

Customize your experience:

```bash
# In interactive mode:
/settings theme cyber        # cyber, minimal, or retro
/settings auto_browse true   # Auto-search when needed
/settings agentic_mode true  # Enable agent features
/settings markdown_enabled true
/settings stream_output true
/settings memory_enabled true
```

## ğŸ—ï¸ Architecture

```
borgo-ai/
â”œâ”€â”€ main.py          # CLI entry point
â”œâ”€â”€ config.py        # Configuration settings
â”œâ”€â”€ llm.py           # Ollama LLM integration
â”œâ”€â”€ embeddings.py    # Vector embeddings + FAISS
â”œâ”€â”€ browser.py       # Web search & scraping (GET only!)
â”œâ”€â”€ rag.py           # Retrieval Augmented Generation
â”œâ”€â”€ memory.py        # Conversation & long-term memory
â”œâ”€â”€ user.py          # User management
â”œâ”€â”€ agent.py         # Agentic AI capabilities (12 tools)
â”œâ”€â”€ ui.py            # Rich terminal UI
â”œâ”€â”€ files.py         # Document loading (PDF, DOCX, etc.)
â”œâ”€â”€ executor.py      # Safe code execution
â”œâ”€â”€ images.py        # Image viewing & info
â”œâ”€â”€ summarizer.py    # Text summarization
â”œâ”€â”€ export.py        # HTML/Markdown/JSON export
â””â”€â”€ data/            # User data storage
    â”œâ”€â”€ users/       # Per-user data
    â””â”€â”€ knowledge/   # RAG knowledge base
```

## ğŸ¯ System Requirements

**Recommended:**
- GPU: RTX 3060 Ti or better (8GB+ VRAM)
- RAM: 16GB+
- Storage: 10GB for models

**Minimum:**
- GPU: Any CUDA-capable GPU with 6GB+ VRAM
- RAM: 12GB
- CPU-only mode works but is slower

## ğŸ› ï¸ Hardware Optimization

The app is optimized for your RTX 3060 Ti:

- **Context window**: Limited to 8K tokens (vs 128K max) for memory efficiency
- **Embeddings**: Uses `nomic-embed-text` (768 dimensions) - fast and efficient
- **FAISS**: CPU version by default (GPU optional)
- **Streaming**: Token-by-token output for better UX

## ğŸ“ Examples

### Basic Chat
```
You: What's the best way to learn Python?

Borgo-AI: Here are my top recommendations for learning Python...
```

### Web Search
```
/search latest AI developments 2024
```

### Agent Mode
```
/agent Find the current price of Bitcoin and compare it to last month
```

### Knowledge Base
```
# Add documentation
/knowledge add Python's GIL (Global Interpreter Lock) prevents multiple native threads from executing Python bytecodes at once...

# Later, ask about it
You: What is Python's GIL?
# The AI will use your added knowledge!
```

### Memory
```
/remember My favorite programming language is Rust
/remember I'm working on a web scraping project

# Later...
You: What project am I working on?
Borgo-AI: Based on my memories, you're working on a web scraping project!
```

## ğŸ”’ Privacy

- **100% Local**: No data leaves your machine
- **No API Keys**: Powered by Ollama, completely free
- **Your Data**: All conversations and memories stored locally in `./data`

## ğŸ› Troubleshooting

**"Cannot connect to Ollama"**
```bash
# Make sure Ollama is running
ollama serve
```

**"Model not found"**
```bash
# Pull the required models
ollama pull llama3.1:8b
ollama pull nomic-embed-text
```

**Out of memory**
- Reduce `context_window` in `config.py`
- Use a smaller model: `ollama pull llama3.1:7b`

**Slow responses**
- Ensure GPU is being used: `nvidia-smi`
- Check Ollama is using GPU: model loads should mention CUDA

## ğŸ“œ License

MIT License - feel free to modify and use as you wish!

---

**Made with â¤ï¸ for local AI enthusiasts**
